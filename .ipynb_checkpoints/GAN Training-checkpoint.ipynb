{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7LJWpPHtJWcX"
   },
   "source": [
    "**Handling all the imports and setting up cuda runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XjVcng2y4mFJ"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rZd7U9EIJzN3"
   },
   "outputs": [],
   "source": [
    "# The size of batches used for training and generation\n",
    "batch_size = 100\n",
    "\n",
    "# The dimension of the noise vector used for generation\n",
    "noise_dim = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WbZ-pVssJJX_"
   },
   "source": [
    "**Loading the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2axHujPGJFfQ"
   },
   "outputs": [],
   "source": [
    "t = torchvision.transforms.Compose([torchvision.transforms.ToTensor(), torchvision.transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "train_dataset = torchvision.datasets.MNIST('/files/', train=True, download=True, transform=t)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset,  batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, .04)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        nn.init.normal_(m.weight.data, 1.0, .04)\n",
    "        nn.init.constant_(m.bias.data, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load one of the following GAN architectures. The first one is a vanilla GAN, the second one is 2 hidden layer DCGAN, and the last one is a 3-hidden layer DCGAN **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Vanilla GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(Generator, self).__init__()\n",
    "    self.fc1 = nn.Linear(100, 256)\n",
    "    self.fc2 = nn.Linear(256, 512)\n",
    "    self.fc3 = nn.Linear(512, 1024)\n",
    "    self.fc4 = nn.Linear(1024, 784)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = F.relu(self.fc1(x))\n",
    "    x = F.relu(self.fc2(x))\n",
    "    x = F.relu(self.fc3(x))\n",
    "    x = torch.tanh(self.fc4(x)).reshape(-1, 1, 28, 28)\n",
    "    return x\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.fc1 = nn.Linear(784, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 512)\n",
    "        self.fc3 = nn.Linear(512, 256)\n",
    "        self.fc4 = nn.Linear(256, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.reshape(-1, 784)\n",
    "        x = F.leaky_relu(self.fc1(x), 0.2)\n",
    "        x = F.dropout(x, 0.3)\n",
    "        x = F.leaky_relu(self.fc2(x), 0.2)\n",
    "        x = F.dropout(x, 0.3)\n",
    "        x = F.leaky_relu(self.fc3(x), 0.2)\n",
    "        x = F.dropout(x, 0.3)\n",
    "        return torch.sigmoid(self.fc4(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2 hidden-layer DCGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CEYhD_lI6ABJ"
   },
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(Generator, self).__init__()\n",
    "    self.conv1 = nn.ConvTranspose2d(noise_dim, 256, kernel_size=7, bias=False)\n",
    "    self.conv2 = nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1, bias=False)\n",
    "    self.conv3 = nn.ConvTranspose2d(128, 1, kernel_size=4, stride=2, padding=1, bias=False)\n",
    "    self.batch1 = nn.BatchNorm2d(256)\n",
    "    self.batch2 = nn.BatchNorm2d(128)\n",
    "  \n",
    "  def forward(self, x):\n",
    "    x = F.relu(self.batch1(self.conv1(x.reshape(-1, noise_dim, 1, 1))))\n",
    "    x = F.relu(self.batch2(self.conv2(x)))\n",
    "    x = torch.tanh(self.conv3(x))\n",
    "    return x\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(Discriminator, self).__init__()\n",
    "    self.conv1 = nn.Conv2d(1, 32, kernel_size=4, stride=2, padding=1, bias=False)\n",
    "    self.batch1 = nn.BatchNorm2d(32)\n",
    "    self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=1, bias=False)\n",
    "    self.batch2 = nn.BatchNorm2d(64)\n",
    "    self.fc1 = nn.Linear(3136, 1, bias=False)\n",
    "  \n",
    "  def forward(self, x):\n",
    "    x = F.leaky_relu(self.batch1(self.conv1(x)), 0.2)\n",
    "    x = F.leaky_relu(self.batch2(self.conv2(x)), 0.2).reshape(-1, 3136)\n",
    "    x = torch.sigmoid(self.fc1(x))\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3 hidden-layer DCGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(Generator, self).__init__()\n",
    "    self.conv1 = nn.ConvTranspose2d(100, 256, kernel_size=4, bias=False)\n",
    "    self.conv2 = nn.ConvTranspose2d(256, 256, kernel_size=4, stride=2, padding=1, bias=False)\n",
    "    self.conv3 = nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1, bias=False)\n",
    "    self.conv4 = nn.ConvTranspose2d(128, 1, kernel_size=4, stride=2, padding=3, bias=False)\n",
    "    self.batch1 = nn.BatchNorm2d(256)\n",
    "    self.batch2 = nn.BatchNorm2d(256)\n",
    "    self.batch3 = nn.BatchNorm2d(128)\n",
    "  \n",
    "  def forward(self, x):\n",
    "    x = F.relu(self.batch1(self.conv1(x.reshape(-1, 100, 1, 1))))\n",
    "    x = F.relu(self.batch2(self.conv2(x)))\n",
    "    x = F.relu(self.batch3(self.conv3(x)))\n",
    "    x = torch.tanh(self.conv4(x))\n",
    "    return x\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(Discriminator, self).__init__()\n",
    "    self.conv1 = nn.Conv2d(1, 32, kernel_size=4, stride=2, padding=1, bias=False)\n",
    "    self.batch1 = nn.BatchNorm2d(32)\n",
    "    self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=1, bias=False)\n",
    "    self.batch2 = nn.BatchNorm2d(64)\n",
    "    self.fc1 = nn.Linear(3136, 256, bias=False)\n",
    "    self.fc2 = nn.Linear(256, 1, bias=False)\n",
    "  \n",
    "  def forward(self, x):\n",
    "    x = F.leaky_relu(self.batch1(self.conv1(x)), 0.2)\n",
    "    x = F.leaky_relu(self.batch2(self.conv2(x)), 0.2).reshape(-1, 3136)\n",
    "    x = F.leaky_relu(self.fc1(x))\n",
    "    x = torch.sigmoid(self.fc2(x))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fzdzsFdeJpTF"
   },
   "outputs": [],
   "source": [
    "G = Generator().to(device)\n",
    "D = Discriminator().to(device)\n",
    "\n",
    "G.apply(weights_init)\n",
    "D.apply(weights_init)\n",
    "\n",
    "# loss function\n",
    "criterion = nn.BCELoss() \n",
    "\n",
    "# optimizer\n",
    "lr = 0.0002\n",
    "optimizerG = optim.Adam(G.parameters(), lr = lr, betas=(0.5, 0.999))\n",
    "optimizerD = optim.Adam(D.parameters(), lr = lr/4, betas=(0.5, 0.999))\n",
    "\n",
    "print(G)\n",
    "print(D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z9rHcBS5Cjgi"
   },
   "outputs": [],
   "source": [
    "def train_Generator(x):\n",
    "    G.zero_grad()\n",
    "\n",
    "    inputs = Variable(torch.randn(batch_size, noise_dim).to(device))\n",
    "    labels = Variable(torch.ones(batch_size, 1).to(device))\n",
    "\n",
    "    G_output = G(inputs)\n",
    "    D_output = D(G_output)\n",
    "    loss = criterion(D_output, labels)\n",
    "\n",
    "    loss.backward()\n",
    "    optimizerG.step()\n",
    "        \n",
    "    return loss.data.item()\n",
    "\n",
    "def train_Discriminator(x):\n",
    "    D.zero_grad()\n",
    "\n",
    "    # training discriminator on real data\n",
    "    real_input, real_labels = x.view(-1, 784), torch.ones(batch_size, 1)*.9\n",
    "    real_input, real_labels = Variable(real_input.to(device).reshape(-1, 1, 28, 28)), Variable(real_labels.to(device))\n",
    "\n",
    "    D_output = D(real_input)\n",
    "    real_loss = criterion(D_output, real_labels)\n",
    "\n",
    "    # training disciminator on fake data\n",
    "    fake_input = Variable(torch.randn(batch_size, noise_dim).to(device))\n",
    "    fake_input, fake_labels = G(fake_input), Variable(torch.zeros(batch_size, 1).to(device))\n",
    "\n",
    "    D_output = D(fake_input)\n",
    "    fake_loss = criterion(D_output, fake_labels)\n",
    "\n",
    "    loss = real_loss + fake_loss\n",
    "    loss.backward()\n",
    "    optimizerD.step()\n",
    "        \n",
    "    return  loss.data.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training loop that trains both the generator and discriminator for each batch of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LPlS2kgmCDFG"
   },
   "outputs": [],
   "source": [
    "n_epoch = 200\n",
    "\n",
    "for epoch in range(1, n_epoch+1):           \n",
    "    D_losses, G_losses = [], []\n",
    "    for batch_idx, x in enumerate(train_loader):\n",
    "        D_losses.append(train_Discriminator(x[0]))\n",
    "        G_losses.append(train_Generator(x[0]))\n",
    "    \n",
    "    print('[%d/%d]: loss_d: %.3f, loss_g: %.3f' % (\n",
    "            (epoch), n_epoch, torch.mean(torch.FloatTensor(D_losses)), torch.mean(torch.FloatTensor(G_losses))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Helper function that displays generated images from the generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VH1vSvWoneOU"
   },
   "outputs": [],
   "source": [
    "import torchvision.utils as vutils\n",
    "\n",
    "def show():\n",
    "  noise = torch.rand(100, 100).to(device)\n",
    "  real_batch = G(noise).detach()\n",
    "  print(real_batch.shape)\n",
    "  plt.figure(figsize=(10,10))\n",
    "  plt.axis(\"off\")\n",
    "  plt.title(\"Generated Images\")\n",
    "  plt.imshow(np.transpose(vutils.make_grid(real_batch.to(device)[:100], nrow=10, padding=2, normalize=True).cpu(),(1,2,0)))\n",
    "\n",
    "show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Copy of beep boop.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
